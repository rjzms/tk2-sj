[
  {
    "id": 1,
    "type": "单选题",
    "q": "下列选项中，不属于Python开发网络爬虫优势的是（）。",
    "opts": {
      "A": "语法简洁，容易上手",
      "B": "开发效率高",
      "C": "丰富的模块",
      "D": "运行速度快、性能强"
    },
    "a": "D",
    "desc": "Python 的解释型特性使得其运行速度通常低于编译型语言（如C/C++），其核心优势在于开发效率高、语法简洁和拥有丰富的第三方库。"
  },
  {
    "id": 2,
    "type": "单选题",
    "q": "下列选项中，被称为主题网络爬虫的是（）。",
    "opts": {
      "A": "增量式网络爬虫",
      "B": "通用网络爬虫",
      "C": "深层爬虫",
      "D": "聚焦网络爬虫"
    },
    "a": "D",
    "desc": "聚焦网络爬虫（Focused Crawler）又称主题网络爬虫，是指选择性地爬取与预先定义的主题相关的页面的网络爬虫。"
  },
  {
    "id": 3,
    "type": "单选题",
    "q": "下列选项中，不属于防爬虫策略的是（）。",
    "opts": {
      "A": "添加User-agent字段",
      "B": "降低访问频率",
      "C": "反复使用同一IP抓取数据",
      "D": "识别验证码"
    },
    "a": "C",
    "desc": "“反复使用同一IP抓取数据”是爬虫的拙劣行为，容易触发网站的反爬机制导致IP被封，它既不是网站的防御策略，也不是爬虫的规避策略（反而是反面教材）。A、B、D均属于爬虫方为了规避反爬而采取的应对策略。"
  },
  {
    "id": 4,
    "type": "单选题",
    "q": "'发布人:张三 发布时间:2022-11-18 来源:图情信息中心'.split(' ')的执行结果是（）。",
    "opts": {
      "A": "['发布人:张三','','发布时间:2022-11-18','','来源:图情信息中心']",
      "B": "['发布人:张三','发布时间:2022-11-18', '来源:图情信息中心']",
      "C": "('发布人:张三','','发布时间:2022-11-18', '', '来源:图情信息中心')",
      "D": "('发布人:张三','发布时间:2022-11-18','来源:图情信息中心')"
    },
    "a": "B",
    "desc": "split() 默认以空格为分隔符，且会自动处理连续的空格，返回一个列表。"
  },
  {
    "id": 5,
    "type": "单选题",
    "q": "关于浏览器加载网页过程的说法，下列描述错误的是（）。",
    "opts": {
      "A": "浏览器通过DNS服务器查找被访问服务器对应的IP地址",
      "B": "浏览器向DNS服务器解析的IP地址发送HTTP请求",
      "C": "Web服务器将响应的HTML页面返回给DNS服务器",
      "D": "浏览器会对HTML页面进行渲染并呈现给用户"
    },
    "a": "C",
    "desc": "Web服务器处理请求后，会将响应的HTML页面直接返回给“浏览器”，而不是返回给DNS服务器。"
  },
  {
    "id": 6,
    "type": "单选题",
    "q": "下列选项中，关于网络爬虫实现技术的描述错误的是（）。",
    "opts": {
      "A": "只有Python语言能够实现爬虫程序",
      "B": "使用Python开发网络爬虫程序效率相对其他语言更高",
      "C": "使用C++语言开发网络爬虫程序代码成型速度慢",
      "D": "Java提供了众多解析网页的技术，对网页解析有着良好的支持"
    },
    "a": "A",
    "desc": "网络爬虫本质是网络请求与数据解析，支持网络编程的语言（如Java、C++、Go、Node.js等）都可以实现，Python 胜在生态丰富和开发效率。"
  },
  {
    "id": 7,
    "type": "单选题",
    "q": "下列选项中，用于解析域名的协议是（）。",
    "opts": {
      "A": "HTTP",
      "B": "DNS",
      "C": "FTP",
      "D": "SMTP"
    },
    "a": "B",
    "desc": "DNS (Domain Name System) 域名系统用于将域名转换为IP地址。"
  },
  {
    "id": 8,
    "type": "单选题",
    "q": "下列选项中，表示超文本传输协议的是（）。",
    "opts": {
      "A": "File",
      "B": "HTTP",
      "C": "FTP",
      "D": "Mailto"
    },
    "a": "B",
    "desc": "HTTP (HyperText Transfer Protocol) 即超文本传输协议。"
  },
  {
    "id": 9,
    "type": "单选题",
    "q": "下列响应头中，用于告诉客户端资源文件的类型和编码的是（）。",
    "opts": {
      "A": "Connection",
      "B": "Content-Encoding",
      "C": "Content-Type",
      "D": "Server"
    },
    "a": "C",
    "desc": "Content-Type 响应头用于指示资源的MIME类型（如 text/html）和字符编码（如 charset=utf-8）。"
  },
  {
    "id": 10,
    "type": "单选题",
    "q": "下列状态码中，表示服务器拒绝访问的是（）。",
    "opts": {
      "A": "402",
      "B": "403",
      "C": "404",
      "D": "405"
    },
    "a": "B",
    "desc": "403 Forbidden 表示服务器理解请求但拒绝授权访问；404 是未找到；405 是方法禁用。"
  },
  {
    "id": 11,
    "type": "单选题",
    "q": "下列选项中，关于聚焦网络爬虫的描述错误的是（）。",
    "opts": {
      "A": "聚焦网络爬虫会随机抓取网页与主题相关的数据",
      "B": "聚焦网络爬虫比通用网络爬虫目的性更强",
      "C": "聚焦网络爬虫会根据一定的网页分析算法对网页进行筛选",
      "D": "聚焦网络爬虫会根据预先设定的主题顺着某个垂直领域进行抓取"
    },
    "a": "A",
    "desc": "聚焦爬虫是有目的、有策略地抓取特定主题的内容，而不是“随机”抓取。"
  },
  {
    "id": 12,
    "type": "单选题",
    "q": "下列选项中，在JSONPath中表示选取根对象的是（）。",
    "opts": {
      "A": "$",
      "B": "/",
      "C": "@",
      "D": "*"
    },
    "a": "A",
    "desc": "在 JSONPath 语法中，$ 表示根对象（Root object），类似于 XPath 中的 /。"
  },
  {
    "id": 13,
    "type": "单选题",
    "q": "下列选项中，关于HTTP协议的描述说法错误的是（）。",
    "opts": {
      "A": "HTTP协议能够高效准确的传送超文本资源",
      "B": "若协议类型为HTTP，则每次连接可以处理多个请求",
      "C": "HTTP协议中的每个请求都是独立的",
      "D": "HTTP协议用于将Web服务器的超文本资源传送到浏览器中"
    },
    "a": "B",
    "desc": "HTTP协议是一种一次性连接，它限制每次连接只处理一个请求，这意味着每个请求都是独立的，当服务器返回本次请求的应答后便立即关闭连接（HTTP 1.1 虽支持 Keep-Alive，但本质设计仍是无状态的）。"
  },
  {
    "id": 14,
    "type": "单选题",
    "q": "以下哪个选项是文件传输协议，访问共享主机的文件资源（）。",
    "opts": {
      "A": "File",
      "B": "FTP",
      "C": "HTTP",
      "D": "Mailto"
    },
    "a": "B",
    "desc": "FTP (File Transfer Protocol) 是文件传输协议。"
  },
  {
    "id": 15,
    "type": "单选题",
    "q": "下列选项中，关于动态页面的描述说法错误的是（）。",
    "opts": {
      "A": "动态网页的内容不一定呈现在网页源代码中",
      "B": "动态网页的访问速度相较于静态网页更快",
      "C": "采用动态网页技术的网站可以实现更多的功能",
      "D": "动态网页相比静态网页，动态网页有数据库支撑"
    },
    "a": "B",
    "desc": "动态网页需要服务器进行计算、查询数据库或客户端渲染 JS，因此访问加载速度通常比内容固定的静态网页要慢。"
  },
  {
    "id": 16,
    "type": "单选题",
    "q": "下列选项中，表示内容类型的字段是（）。",
    "opts": {
      "A": "Cache-Control",
      "B": "Connection",
      "C": "Content-Encoding",
      "D": "Content-Type"
    },
    "a": "D",
    "desc": "A表示缓存控制；B表示连接状态；C表示内容编码（压缩方式）；D表示内容类型。"
  },
  {
    "id": 17,
    "type": "单选题",
    "q": "下列选项中，用于标识客户端身份的是（）。",
    "opts": {
      "A": "HOST",
      "B": "User-Agent",
      "C": "Accept",
      "D": "Referer"
    },
    "a": "B",
    "desc": "User-Agent 请求头包含了浏览器类型、操作系统等信息，服务器通过它来识别客户端身份。"
  },
  {
    "id": 18,
    "type": "单选题",
    "q": "关于响应状态码的描述说法错误的是（）。",
    "opts": {
      "A": "响应状态码代表服务器的响应状态",
      "B": "响应状态码的作用是告知客户端请求Web资源的结果",
      "C": "若服务器发生错误，用户便无法获取响应状态码",
      "D": "当响应状态码为200时表示服务器接收请求并成功处理"
    },
    "a": "C",
    "desc": "即使服务器发生错误，通常也会返回 5xx 系列的状态码（如 500 Internal Server Error），用户是可以获取到的。"
  },
  {
    "id": 19,
    "type": "单选题",
    "q": "下列选项中，关于Requests库post()函数的说法错误的是（）。",
    "opts": {
      "A": "如果请求数据类型为Json可通过参数json传递",
      "B": "post()函数会根据传入的URL构建一个请求并将该请求发送给服务器",
      "C": "post()函数通过参数data携带请求数据",
      "D": "post()函数既可以发送GET请求也可以发送POST请求"
    },
    "a": "D",
    "desc": "post() 函数专门用于发送 POST 请求，get() 函数用于发送 GET 请求。"
  },
  {
    "id": 20,
    "type": "单选题",
    "q": "关于抓取静态网页实现技术的说法，下列描述错误的是（）。",
    "opts": {
      "A": "如果要抓取静态网页的数据，只需要获得网页的源代码即可",
      "B": "通过urllib、urllib3和Requests等库抓取静态网页数据",
      "C": "Requests库只能发送网络请求不能获取网页源码",
      "D": "抓取静态网页数据的整个过程是模仿用户通过浏览器访问网页的过程"
    },
    "a": "C",
    "desc": "Requests 库非常强大，发送请求后，可以通过 `response.text` 或 `response.content` 直接获取网页源代码。"
  },
  {
    "id": 21,
    "type": "单选题",
    "q": "下列选项中，不属于HTML元素组成的是（）。",
    "opts": {
      "A": "开始标签",
      "B": "内容",
      "C": "样式",
      "D": "结束标签"
    },
    "a": "C",
    "desc": "一个标准的 HTML 元素由开始标签、内容和结束标签组成。样式（CSS）通常是元素的属性或外部定义，不属于元素本身的结构组成部分。"
  },
  {
    "id": 22,
    "type": "单选题",
    "q": "下列选项中，表示图像标签的是（）。",
    "opts": {
      "A": "<html>",
      "B": "<h1>",
      "C": "<p>",
      "D": "<img>"
    },
    "a": "D",
    "desc": "<img> 标签用于定义图像。"
  },
  {
    "id": 23,
    "type": "单选题",
    "q": "下列选项中，不属于请求行组成的是（）。",
    "opts": {
      "A": "请求方法",
      "B": "URL",
      "C": "协议版本",
      "D": "请求数据"
    },
    "a": "D",
    "desc": "HTTP请求报文由请求行、请求头、空行和请求体组成。请求行包含：请求方法、URL、协议版本。请求数据位于“请求体”中。"
  },
  {
    "id": 24,
    "type": "单选题",
    "q": "下列选项中，关于静态页面的描述说法错误的是（）。",
    "opts": {
      "A": "静态网页的交互性较差，在功能方面有较大的限制",
      "B": "静态网页的访问速度快，访问过程中无需连接数据库",
      "C": "静态网页没有数据库的支持，内容更新与维护比较复杂",
      "D": "静态网页的内容可根据用户信息进行定制化展示"
    },
    "a": "D",
    "desc": "静态网页内容是固定的，无法根据不同用户的身份动态展示不同的内容（那是动态网页的特征）。"
  },
  {
    "id": 25,
    "type": "单选题",
    "q": "下列选项中，关于处理响应的描述说法错误的是（）。",
    "opts": {
      "A": "当服务器返回的响应状态码为200时，表明可以接收到由服务器返回的响应信息",
      "B": "Response类的对象中封装了服务器返回的响应信息",
      "C": "响应内容中只能包含文本内容",
      "D": "若想获取响应的最终URL，可通过url属性获取"
    },
    "a": "C",
    "desc": "响应内容不仅可以是文本（text），还可以是二进制数据（content），例如图片、音频、视频文件等。"
  },
  {
    "id": 26,
    "type": "单选题",
    "q": "下列选项中，用于在GET请求中传递查询字符串的是（）。",
    "opts": {
      "A": "params",
      "B": "headers",
      "C": "verify",
      "D": "timeout"
    },
    "a": "A",
    "desc": "在 requests.get() 中，`params` 参数用于传递 URL 查询参数（?key=value）。"
  },
  {
    "id": 27,
    "type": "单选题",
    "q": "下列选项中，用于查看响应状态码的属性是（）。",
    "opts": {
      "A": "content",
      "B": "headers",
      "C": "text",
      "D": "status_code"
    },
    "a": "D",
    "desc": "`response.status_code` 用于获取 HTTP 状态码。"
  },
  {
    "id": 28,
    "type": "单选题",
    "q": "下列选项中，关于检测代理IP有效性的描述说法错误的是（）。",
    "opts": {
      "A": "当使用代理访问网站时，返回的状态码为200时表示代理可用",
      "B": "当代理无效时，不能返回响应信息",
      "C": "使用的代理IP通过参数proxies传递",
      "D": "post()函数无法使用代理ip"
    },
    "a": "D",
    "desc": "`post()` 函数和 `get()` 函数一样，都支持 `proxies` 参数来设置代理 IP。"
  },
  {
    "id": 29,
    "type": "单选题",
    "q": "（）会将数据包原封不动地转发给服务器，让服务器认为当前访问的用户只是一个普通客户端，而不是代理服务器。",
    "opts": {
      "A": "高度匿名代理服务器",
      "B": "普通匿名代理服务器",
      "C": "透明代理服务器",
      "D": "所有选项均不对"
    },
    "a": "A",
    "desc": "高度匿名代理会将数据包原封不动转发，不改变请求头，服务器无法察觉使用了代理。"
  },
  {
    "id": 30,
    "type": "单选题",
    "q": "关于定制请求头的描述说法错误的是（）。",
    "opts": {
      "A": "参数headers可以接收列表类型的数据",
      "B": "定制的请求头需要由参数headers中传递",
      "C": "get()函数和post()函数均可以添加定制请求头",
      "D": "定制请求的目的是将发送的请求伪装成浏览器发送的请求"
    },
    "a": "A",
    "desc": "`headers` 参数必须接收**字典 (dictionary)** 类型的数据，键值对对应请求头的字段和值。"
  },
  {
    "id": 31,
    "type": "单选题",
    "q": "阅读代码：response.encoding = 'ISO-8859-1'，print(response.text)。上述程序运行后，会使用哪种编码方式返回文本（）。",
    "opts": {
      "A": "utf-8",
      "B": "gbk",
      "C": "gbk2312",
      "D": "ISO-8859-1"
    },
    "a": "D",
    "desc": "代码显式将 `response.encoding` 设置为了 'ISO-8859-1'，因此访问 `text` 属性时会使用该编码解码。"
  },
  {
    "id": 32,
    "type": "单选题",
    "q": "requests库中，get()函数能用于设置是否启用SSL证书的参数是（）。",
    "opts": {
      "A": "url",
      "B": "headers",
      "C": "verify",
      "D": "proxies"
    },
    "a": "C",
    "desc": "`verify` 参数用于控制是否验证服务器的 SSL 证书，默认为 True。"
  },
  {
    "id": 33,
    "type": "单选题",
    "q": "关于Requests库中get()函数的说法错误的是（）。",
    "opts": {
      "A": "get()函数既可以发送GET请求也可以发送POST请求",
      "B": "get()函数中参数url是必选参数，该参数含义为请求地址",
      "C": "get()函数会根据传入的URL构建一个请求",
      "D": "使用get()函数发送GET请求时可以携带请求参数"
    },
    "a": "A",
    "desc": "`get()` 只能发送 GET 请求，发送 POST 请求需使用 `post()`。"
  },
  {
    "id": 34,
    "type": "单选题",
    "q": "下列选项中，关于Cookie的描述错误的是（）。",
    "opts": {
      "A": "Cookie是一段文本数据，由一个名称和一个值组成",
      "B": "Cookie的生存期可以由开发人员设置",
      "C": "Cookie数据存储在网站服务器中",
      "D": "Cookie是为了网站辨别用户身份、进行会话跟踪而存储的数据"
    },
    "a": "C",
    "desc": "Cookie 数据是存储在**客户端（用户浏览器）**中的，而不是服务器中。Session 才是存储在服务器端的。"
  },
  {
    "id": 35,
    "type": "单选题",
    "q": "下列正则表达式中，表示只能匹配任意数字的是（）。",
    "opts": {
      "A": "\\w",
      "B": "\\s",
      "C": "\\d",
      "D": "\\b"
    },
    "a": "C",
    "desc": "`\\d` 匹配任意数字 [0-9]；`\\w` 匹配字母数字下划线；`\\s` 匹配空白；`\\b` 匹配单词边界。"
  },
  {
    "id": 36,
    "type": "单选题",
    "q": "下列选项中，表示匹配前导字符0次或1次的是（）。",
    "opts": {
      "A": "?",
      "B": "*",
      "C": "+",
      "D": "{n}"
    },
    "a": "A",
    "desc": "`?` 表示匹配 0 次或 1 次；`*` 是 0 次或多次；`+` 是 1 次或多次。"
  },
  {
    "id": 37,
    "type": "单选题",
    "q": "re模块中，对正则表达式进行预编译，从而生成一个代表正则表达式的Pattern对象的方法是（）。",
    "opts": {
      "A": "re.pattern()",
      "B": "re.split()",
      "C": "re.run()",
      "D": "re.compile()"
    },
    "a": "D",
    "desc": "`re.compile()` 用于编译正则表达式模式，返回一个 Pattern 对象。"
  },
  {
    "id": 38,
    "type": "单选题",
    "q": "关于正则表达式的描述，说法错误的是（）。",
    "opts": {
      "A": "一条正则表达式也称为一个模式",
      "B": "正则表达式匹配HTML时会根据其层次结构进行匹配",
      "C": "正则表达式由普通字符、元字符或预定义字符集组成",
      "D": "正则表达式是对字符串操作的一种逻辑公式"
    },
    "a": "B",
    "desc": "正则表达式将 HTML 视为纯文本字符串，它**忽略** HTML 的 DOM 层次结构，只是进行字符模式匹配。"
  },
  {
    "id": 39,
    "type": "单选题",
    "q": "下列选项中，关于设置代理服务器目的的说法正确的是（）。",
    "opts": {
      "A": "加快网络爬虫抓取数据的速度",
      "B": "识别网站验证码",
      "C": "降低访问网站速度",
      "D": "防止IP被封禁"
    },
    "a": "D",
    "desc": "使用代理 IP 的主要目的是为了隐藏真实 IP，防止因频繁访问被目标网站封禁。通常代理会增加网络延迟，不一定能加快速度。"
  },
  {
    "id": 40,
    "type": "单选题",
    "q": "XPath路径表达式中，在搜索节点时会忽略层级关系的是（）。",
    "opts": {
      "A": "/",
      "B": "//",
      "C": "[]",
      "D": "@"
    },
    "a": "B",
    "desc": "`//` 表示从当前节点选择文档中的节点，而不考虑它们的位置（后代节点）。`/` 表示直接子节点。"
  },
  {
    "id": 41,
    "type": "单选题",
    "q": "关于XPath的描述，说法错误的是（）。",
    "opts": {
      "A": "XPath基于XML或HTML的节点树定位目标节点所在的位置",
      "B": "XPath是一种用于确定XML文档中部分节点位置的语言",
      "C": "XPath匹配节点的方式与正则表达式匹配字符串的方式类似",
      "D": "XPath通过路径表达式可以快速地定位与选取XML或HTML文档中的一个节点或者一组节点集"
    },
    "a": "C",
    "desc": "XPath 是基于 DOM 树结构的路径选择语言，而正则表达式是基于字符流的模式匹配，两者的原理完全不同。"
  },
  {
    "id": 42,
    "type": "单选题",
    "q": "img标签中的什么属性，用于指图片地址（）。",
    "opts": {
      "A": "src",
      "B": "href",
      "C": "title",
      "D": "alt"
    },
    "a": "A",
    "desc": "`src` (source) 属性规定图像的 URL。"
  },
  {
    "id": 43,
    "type": "单选题",
    "q": "当正则表达式中包含能接受重复的限定符时，匹配尽可能少的字符，这被称为（）。",
    "opts": {
      "A": "贪婪匹配",
      "B": "懒惰匹配",
      "C": "占有匹配",
      "D": "随机匹配"
    },
    "a": "B",
    "desc": "尽可能少匹配称为“懒惰匹配”或“非贪婪匹配”；尽可能多匹配称为“贪婪匹配”。"
  },
  {
    "id": 44,
    "type": "单选题",
    "q": "正则表达式[a-z].*3可以匹配abc3abc3a3几次（）。",
    "opts": {
      "A": "0",
      "B": "1",
      "C": "2",
      "D": "3"
    },
    "a": "B",
    "desc": "`.*` 默认是贪婪匹配，它会尽可能多地吞掉字符，直到最后一个 `3`，所以整个字符串只会被匹配为 1 次结果。"
  },
  {
    "id": 45,
    "type": "单选题",
    "q": "下列选项中，表示匹配的字符串开头元字符是（）。",
    "opts": {
      "A": "*",
      "B": "^",
      "C": "$",
      "D": "[]"
    },
    "a": "B",
    "desc": "`^` 匹配字符串的开头；`$` 匹配结尾。"
  },
  {
    "id": 46,
    "type": "单选题",
    "q": "能将'baidu_logo.png'正确的保存到文件中的代码是（）。",
    "opts": {
      "A": "with open('baidu_logo.png', 'wb') as file:",
      "B": "with open('baidu_logo.png', 'w') as file:",
      "C": "with open('baidu_logo.png', 'wr') as file:",
      "D": "with open('baidu_logo.png', 'a+') as file:"
    },
    "a": "A",
    "desc": "图片属于二进制文件，写入时必须使用 `'wb'` (write binary) 模式，否则会报错或损坏。"
  },
  {
    "id": 47,
    "type": "单选题",
    "q": "下列选项中，BeautifulSoup使用CSS选择器的方法是（）。",
    "opts": {
      "A": "search()",
      "B": "findall()",
      "C": "find()",
      "D": "select()"
    },
    "a": "D",
    "desc": "BeautifulSoup 中使用 `select()` 方法来支持 CSS 选择器语法。"
  },
  {
    "id": 48,
    "type": "单选题",
    "q": "使用xpath获取文本使用（）。",
    "opts": {
      "A": "text",
      "B": "text()",
      "C": "content",
      "D": "content()"
    },
    "a": "B",
    "desc": "在 XPath 中，使用 `text()` 函数来选取节点的文本内容。"
  },
  {
    "id": 49,
    "type": "单选题",
    "q": "下列XPath路径表达式中。用于选取第一个app元素的是（）。",
    "opts": {
      "A": "/appstore/app(1)",
      "B": "/appstore/app(first)",
      "C": "/appstore/app[1]",
      "D": "/appstore/app[first]"
    },
    "a": "C",
    "desc": "XPath 的位置索引是从 **1** 开始的，使用方括号 `[]`，所以是 `app[1]`。"
  },
  {
    "id": 50,
    "type": "单选题",
    "q": "下列选项中，属于Selenium访问指定URL地址的方法是（）。",
    "opts": {
      "A": "get()",
      "B": "post()",
      "C": "head()",
      "D": "put()"
    },
    "a": "A",
    "desc": "Selenium 使用 `driver.get(url)` 来打开网页。"
  },
  {
    "id": 51,
    "type": "单选题",
    "q": "下列选项中，关于Selenium的描述说法错误的是（）。",
    "opts": {
      "A": "Selenium是一个开源的、便携式的自动化测试工具",
      "B": "Selenium可以模拟用户使用浏览器完成一些动作",
      "C": "Selenium最初的目的是为了便于网络爬虫抓取动态网页数据",
      "D": "Selenium需要通过浏览器驱动程序WebDriver才能与所选浏览器进行交互"
    },
    "a": "C",
    "desc": "Selenium 最初是作为 **Web 应用自动化测试工具** 开发的，后来因为其强大的浏览器控制能力才被广泛用于爬虫。"
  },
  {
    "id": 52,
    "type": "单选题",
    "q": "关于jsonpath模块的描述，说法错误的是（）。",
    "opts": {
      "A": "jsonpath是一个解析JSON文档的模块",
      "B": "jsonpath()函数根据JSONPath的表达式定位目标对象",
      "C": "jsonpath函数会返回包含解析后的结果的列表",
      "D": "jsonpath模块可以解析XML文档中的数据"
    },
    "a": "D",
    "desc": "jsonpath 顾名思义，只能解析 JSON 数据，无法解析 XML（XML 使用 XPath）。"
  },
  {
    "id": 53,
    "type": "单选题",
    "q": "下列选项中，关于网络爬虫合法性探究的描述说法错误的是（）。",
    "opts": {
      "A": "Robots协议又称爬虫协议",
      "B": "Robots协议能够有效防范网络爬虫",
      "C": "爬虫会给网站增加不小的压力",
      "D": "Robots协议没有实际的约束力"
    },
    "a": "B",
    "desc": "Robots 协议只是一个“君子协定”，告诉爬虫哪些页面不应抓取，但它没有技术上的强制约束力，不能物理上“防范”恶意爬虫。"
  },
  {
    "id": 54,
    "type": "单选题",
    "q": "以下关于列表操作的描述,错误的是（）。",
    "opts": {
      "A": "通过 append 方法可以向列表添加元素",
      "B": "通过 extend 方法可以将另一个列表中的元素逐一添加到列表中",
      "C": "通过insert(index, object)方法,在指定位置index前插入元素",
      "D": "通过 add 方法可以向列表添加元素"
    },
    "a": "D",
    "desc": "Python 列表（List）没有 `add` 方法，`add` 是集合（Set）的方法。"
  },
  {
    "id": 55,
    "type": "单选题",
    "q": "下列选项中，关于设置代理服务器的描述错误的是（）。",
    "opts": {
      "A": "降低单个IP访问频率",
      "B": "防止IP被封禁",
      "C": "加快访问网站的速度",
      "D": "代理IP的寿命是有限的"
    },
    "a": "C",
    "desc": "使用代理服务器通常会因为多了一次数据中转而**增加延迟**，一般不会加快访问速度（除非直连受限）。"
  },
  {
    "id": 56,
    "type": "单选题",
    "q": "以下 Python 语言关键字在异常处理结构中用来捕获特定类型异常的选项是（）。",
    "opts": {
      "A": "for",
      "B": "lambda",
      "C": "in",
      "D": "except"
    },
    "a": "D",
    "desc": "Python 异常处理结构为 `try...except...`，其中 `except` 用于捕获异常。"
  },
  {
    "id": 57,
    "type": "单选题",
    "q": "在匹配嵌套了HTML内容的文本时，会忽略HTML内容本身存在的层次结构的解析语言是（）。",
    "opts": {
      "A": "正则表达式",
      "B": "XPath",
      "C": "Beautiful Soup",
      "D": "所有选项均正确"
    },
    "a": "A",
    "desc": "正则表达式将文本视为线性字符流，不具备 DOM 树的层次理解能力。"
  },
  {
    "id": 58,
    "type": "单选题",
    "q": "demo_dict = {\"city\": \"北京\", \"name\": \"小明\"}。print(json.dumps(demo_dict, ensure_ascii=False)) 运行结果为（）。",
    "opts": {
      "A": "{\"city\": \"北京\", \"name\": \"小明\"}",
      "B": "{\"city\": \"\\u5317\\u4eac\", \"name\": \"\\u5c0f\\u660e\"}",
      "C": "{}",
      "D": "运行错误"
    },
    "a": "A",
    "desc": "`ensure_ascii=False` 参数的作用就是让 JSON 序列化时直接输出非 ASCII 字符（如中文），而不是输出 Unicode 转义序列。"
  },
  {
    "id": 59,
    "type": "单选题",
    "q": "https的端口号是（）。",
    "opts": {
      "A": "80",
      "B": "8080",
      "C": "443",
      "D": "433"
    },
    "a": "C",
    "desc": "HTTP 默认端口 80，HTTPS 默认端口 443。"
  },
  {
    "id": 60,
    "type": "单选题",
    "q": "下列哪个正则表达式与 1\\d{5,9}不相同（）。",
    "opts": {
      "A": "[1]\\d{5,9}",
      "B": "1[0-9]{5,9}",
      "C": "1[0123456789]{5,9}",
      "D": "[1]\\D{5,9}"
    },
    "a": "D",
    "desc": "`\\d` 表示数字，`\\D` 表示非数字。D 选项匹配的是“1后面跟5-9个非数字”，显然与原题意相反。"
  },
  {
    "id": 61,
    "type": "单选题",
    "q": "下列不能匹配任意字符的正则表达式是（）。",
    "opts": {
      "A": "[\\d\\D]",
      "B": "[\\w\\W]",
      "C": "[\\s\\S]",
      "D": "[\\a\\A]"
    },
    "a": "D",
    "desc": "`\\a` 不是标准的元字符（通常是响铃符），`[\\a\\A]` 无法像前三项那样利用互补集匹配所有字符。"
  },
  {
    "id": 62,
    "type": "单选题",
    "q": "下列选项中，表示向服务器提交表单或上传文件的请求方法是（）。",
    "opts": {
      "A": "GET",
      "B": "POST",
      "C": "HEAD",
      "D": "PUT"
    },
    "a": "B",
    "desc": "POST 请求常用于提交数据（表单、文件）。"
  },
  {
    "id": 63,
    "type": "单选题",
    "q": "下列选项中，用于以二进制形式获取响应内容的属性是（）。",
    "opts": {
      "A": "status_code",
      "B": "text",
      "C": "content",
      "D": "string"
    },
    "a": "C",
    "desc": "`response.content` 返回二进制数据（bytes），`response.text` 返回解码后的字符串。"
  },
  {
    "id": 64,
    "type": "单选题",
    "q": "bs4中，若已找到节点并存放于变量x中，能获取节点内容的是（）。",
    "opts": {
      "A": "x.text",
      "B": "x.content",
      "C": "x.html",
      "D": "x.attrs"
    },
    "a": "A",
    "desc": "BeautifulSoup 对象使用 `.text` 或 `.get_text()` 获取纯文本内容。"
  },
  {
    "id": 65,
    "type": "单选题",
    "q": "以下XPath谓语中，能获得满足条件的第一个节点的是（）。",
    "opts": {
      "A": "[0]",
      "B": "[1]",
      "C": "[first()]",
      "D": "[min()+1]"
    },
    "a": "B",
    "desc": "XPath 索引从 1 开始，所以 `[1]` 表示第一个节点。"
  },
  {
    "id": 66,
    "type": "单选题",
    "q": "lxml库中，用于解析xml文件的方法是（）。",
    "opts": {
      "A": "etree.parse()",
      "B": "etree.XML()",
      "C": "etree.HTML()",
      "D": "etree.fromstring()"
    },
    "a": "A",
    "desc": "`parse()` 用于解析文件对象或路径，其他方法用于解析字符串。"
  },
  {
    "id": 67,
    "type": "单选题",
    "q": "关于JSONPath的描述，说法错误的是（）。",
    "opts": {
      "A": "JSONPath只适用于JSON文档",
      "B": "JSONPath提供了描述JSON文档层次结构的表达式",
      "C": "JSONPath提供的语法与XPath提供的语法相同",
      "D": "JSONPath可以看作定位目标对象位置的语言"
    },
    "a": "C",
    "desc": "JSONPath 借鉴了 XPath 的概念，但**语法不相同**（例如根节点符是 `$` 而不是 `/`，数组下标用 `[]` 等）。"
  },
  {
    "id": 68,
    "type": "单选题",
    "q": "requests库中，proxies参数传入一个字典，该字典中包含了所需要的代理IP，其中字典的键为（）。",
    "opts": {
      "A": "get",
      "B": "ip地址",
      "C": "协议类型（http或https）",
      "D": "post"
    },
    "a": "C",
    "desc": "proxies 字典的键应为协议名，如 `{'http': '...', 'https': '...'}`。"
  },
  {
    "id": 69,
    "type": "单选题",
    "q": "URL地址'https://www.baidu.com?ie=utf-8&wd=python'，其中属于表示查询字符串的是（）。",
    "opts": {
      "A": "ie=utf-8&wd=python",
      "B": "https",
      "C": "www.baidu.com",
      "D": "wd=python"
    },
    "a": "A",
    "desc": "查询字符串（Query String）位于 URL 的 `?` 之后，包含参数键值对。"
  },
  {
    "id": 70,
    "type": "单选题",
    "q": "关于CSS选择器的描述，说法错误的是（）。",
    "opts": {
      "A": "类别选择器是根据类名选择元素，类名前面用“.”进行标注",
      "B": "ID选择器是根据特定ID选择元素，ID前面加上“$”进行标注",
      "C": "属性选择器是根据元素的属性选择元素，属性必须用中括号进行包裹",
      "D": "元素选择器是根据元素名称选择元素"
    },
    "a": "B",
    "desc": "ID 选择器应使用井号 **`#`** 进行标注，而不是 `$`。"
  },
  {
    "id": 71,
    "type": "单选题",
    "q": "以下选项中是HTTP请求行的是（）。",
    "opts": {
      "A": "GET / HTTP/1.1",
      "B": "Connection: keep-alive",
      "C": "Accept-Language: zh-CN,zh;q=0.9",
      "D": "User-Agent: Mozilla/5.0 ..."
    },
    "a": "A",
    "desc": "请求行包含请求方法、URL路径和协议版本。B、C、D 都是请求头。"
  },
  {
    "id": 72,
    "type": "单选题",
    "q": "GET请求方法通过请求参数传输数据，最多能传输的数据量是（）。",
    "opts": {
      "A": "2KB",
      "B": "4KB",
      "C": "1M",
      "D": "无限制"
    },
    "a": "A",
    "desc": "虽然 HTTP 协议本身未限制，但主流浏览器和服务器通常将 GET 请求的 URL 长度限制在 2KB - 8KB 左右。"
  },
  {
    "id": 73,
    "type": "单选题",
    "q": "关于Beautiful Soup的描述，说法错误的是（）。",
    "opts": {
      "A": "Beautiful Soup是一个用于从HTML或XML文档中提取目标数据的Python库",
      "B": "Beautiful Soup支持CSS选择器",
      "C": "Beautiful Soup可以将HTML或XML文档、片段转换成节点树",
      "D": "Beautiful Soup会将整个节点树看作一个Python类的对象"
    },
    "a": "D",
    "desc": "BeautifulSoup 将 HTML 文档转换为一个复杂的树形结构，每个**节点**都是 Python 对象，而不是只把“整个树”看作一个对象。"
  },
  {
    "id": 74,
    "type": "单选题",
    "q": "selenium中，浏览器对象往网页的输入框中输入文字需要调用的方法是（）。",
    "opts": {
      "A": "get()",
      "B": "save_screenshot()",
      "C": "send_keys()",
      "D": "find_element_by_id()"
    },
    "a": "C",
    "desc": "`send_keys()` 方法用于向元素发送按键输入。"
  },
  {
    "id": 75,
    "type": "单选题",
    "q": "selenium中，用于关闭浏览器对象的是（）。",
    "opts": {
      "A": "driver.cancel()",
      "B": "driver.exit()",
      "C": "driver.quit()",
      "D": "driver.close()"
    },
    "a": "C",
    "desc": "`quit()` 会关闭所有关联窗口并结束 WebDriver 会话；`close()` 仅关闭当前窗口。"
  },
  {
    "id": 76,
    "type": "单选题",
    "q": "selenium中，能查找 <form name='hello'></form> 的元素的表达式是（）。",
    "opts": {
      "A": "find_element_by_css_selector('hello')",
      "B": "find_element_by_class_name('hello')",
      "C": "find_element_by_tag_name('hello')",
      "D": "find_element_by_name('hello')"
    },
    "a": "D",
    "desc": "根据 name 属性查找，应使用 `find_element_by_name` (或 `By.NAME`)。"
  },
  {
    "id": 77,
    "type": "单选题",
    "q": "下列选项中，关于进程的描述错误的是( )。",
    "opts": {
      "A": "进程是系统进行资源分配的最小单位",
      "B": "进程拥有自己的内存空间",
      "C": "进程之间数据不共享",
      "D": "进程的存在必须依赖线程"
    },
    "a": "D",
    "desc": "早期操作系统只有进程，进程是独立存在的，并不依赖线程。线程是后来引入的轻量级进程。"
  },
  {
    "id": 78,
    "type": "单选题",
    "q": "下列选项中，关于多线程爬虫的述错误的是( )。",
    "opts": {
      "A": "开启的线程数量越多，程序运行速度越快",
      "B": "多线程爬虫可以有效利用CPU和网络I/O等待时间，提高数据采集效率",
      "C": "多线程爬虫使用队列是为了保证安全地使用多线程采集网页数据",
      "D": "通常情况下，多程爬虫会开启多个线程抓取网页和解析网页"
    },
    "a": "A",
    "desc": "线程开启过多会增加 CPU 调度和上下文切换的开销，且受限于带宽和目标服务器限制，速度不一定成正比，甚至可能变慢或被封。"
  },
  {
    "id": 79,
    "type": "单选题",
    "q": "以下哪个是多线程爬虫中，用于等待子线程结束的方法？( )",
    "opts": {
      "A": "start()",
      "B": "join()",
      "C": "setDaemon()",
      "D": "run()"
    },
    "a": "B",
    "desc": "`join()` 方法用于阻塞主线程，直到调用该方法的子线程执行结束。"
  },
  {
    "id": 80,
    "type": "单选题",
    "q": "在Python中，以下哪个选项用于创建一个线程对象？( )",
    "opts": {
      "A": "threading.create_thread()",
      "B": "threading.Thread()",
      "C": "threading.new_thread()",
      "D": "threading.start_thread()"
    },
    "a": "B",
    "desc": "`threading.Thread()` 是创建线程对象的标准类。"
  },
  {
    "id": 81,
    "type": "单选题",
    "q": "在Python中，通过继承Thread类创建自定义线程时，需要重写哪个方法？( )",
    "opts": {
      "A": "__init__()",
      "B": "start()",
      "C": "run()",
      "D": "execute()"
    },
    "a": "C",
    "desc": "继承 Thread 类后，必须重写 `run()` 方法来定义线程要执行的任务。"
  },
  {
    "id": 82,
    "type": "单选题",
    "q": "下列选项中，关于Scrapy框架的描述正确的是（ ）。",
    "opts": {
      "A": "Scrapy是一个纯使用Python语言开发的收费的网络爬虫框架",
      "B": "Scrapy支持Shell工具，方便开发人员独立调试程序",
      "C": "Scrapy自身可以实现分布式爬虫",
      "D": "Scrapy是基于Scrapy-Splash框架开发的"
    },
    "a": "B",
    "desc": "A错误（开源免费）；C错误（需配合 scrapy-redis 实现分布式）；D错误（Scrapy-Splash 是基于 Scrapy 的插件）。"
  },
  {
    "id": 83,
    "type": "判断题",
    "q": "爬虫的合法性完全取决于网站的robots.txt文件。",
    "a": "错",
    "desc": "Robots 协议只是行业规范，爬虫的合法性更取决于当地法律法规（如是否侵犯个人隐私、知识产权、非法获取计算机信息系统数据等）。"
  },
  {
    "id": 84,
    "type": "判断题",
    "q": "互联网上每个文件都有一个唯一的URL。",
    "a": "对",
    "desc": "URL (Uniform Resource Locator) 统一资源定位符，旨在唯一标识网络上的资源。"
  },
  {
    "id": 85,
    "type": "判断题",
    "q": "一个IP地址只能对应一个域名。",
    "a": "错",
    "desc": "通过虚拟主机技术，一个 IP 地址可以对应成千上万个域名。"
  },
  {
    "id": 86,
    "type": "判断题",
    "q": "HTTP协议是无状态的，这意味着每次请求都是独立的，不会记住之前的请求信息。",
    "a": "对",
    "desc": "这是 HTTP 协议的核心设计原则，虽然可以通过 Cookie/Session 模拟状态，但协议本身是无状态的。"
  },
  {
    "id": 87,
    "type": "判断题",
    "q": "爬虫在抓取动态内容时，如果直接使用requests库获取网页源码，能够获得所有渲染后的内容。",
    "a": "错",
    "desc": "Requests 只能获取服务器返回的原始 HTML 文本，无法执行 JavaScript，因此无法获取 JS 动态渲染的内容。"
  },
  {
    "id": 88,
    "type": "判断题",
    "q": "get方法比post方法速度更快。",
    "a": "对",
    "desc": "通常情况下，GET 请求包体更小，且无需像 POST 那样包含复杂的请求体处理，速度略快。"
  },
  {
    "id": 89,
    "type": "判断题",
    "q": "一次HTTP通信的过程包括HTTP请求和HTTP响应。",
    "a": "对",
    "desc": "请求-响应模式是 HTTP 通信的基础。"
  },
  {
    "id": 90,
    "type": "判断题",
    "q": "在HTTP请求中，GET方法比POST方法更适合传输大量数据。",
    "a": "错",
    "desc": "GET 请求受 URL 长度限制，适合少量数据；POST 请求数据在 Body 中，无大小限制，适合大量数据。"
  },
  {
    "id": 91,
    "type": "判断题",
    "q": "在HTTP请求头中，Referer字段会告知服务器请求的来源页面，能帮助网站分析流量来源。",
    "a": "对",
    "desc": "Referer 记录了发起当前请求的来源页面地址。"
  },
  {
    "id": 92,
    "type": "判断题",
    "q": "当服务器返回HTTP状态码404时，表示请求的资源没有找到。",
    "a": "对",
    "desc": "404 Not Found 是最常见的客户端错误码。"
  },
  {
    "id": 93,
    "type": "判断题",
    "q": "爬虫可以通过模拟点击事件来动态获取数据，但如果没有正确处理JavaScript渲染，它仍然无法抓取数据。",
    "a": "对",
    "desc": "模拟点击通常需要 JS 执行环境（如 Selenium），纯 HTTP 请求无法触发 JS 事件。"
  },
  {
    "id": 94,
    "type": "判断题",
    "q": "Python中的open()函数可以用于打开网页并读取网页内容。",
    "a": "错",
    "desc": "`open()` 只能操作本地文件系统。读取网页需要网络库（如 `requests`, `urllib`）。"
  },
  {
    "id": 95,
    "type": "判断题",
    "q": "requests.get()方法可以用来发送GET请求并返回一个包含响应内容的对象。",
    "a": "对",
    "desc": ""
  },
  {
    "id": 96,
    "type": "判断题",
    "q": "在XPath中，//input[@type='text']可以选择所有type='text'的input元素，无论其层级如何。",
    "a": "对",
    "desc": "`//` 表示全文档扫描，忽略层级。"
  },
  {
    "id": 97,
    "type": "判断题",
    "q": "在XPath中，//a[starts-with(@href, 'https')]会选择所有href属性以https开头的a标签。",
    "a": "对",
    "desc": "`starts-with()` 是 XPath 的标准函数。"
  },
  {
    "id": 98,
    "type": "判断题",
    "q": "在XPath中，//div[@id='content']/text()可以选取id='content'的div标签中的所有文本节点。",
    "a": "错",
    "desc": "`text()` 只能选取当前节点的直接子文本，不包含后代元素（如子 `<span>`）中的文本。"
  },
  {
    "id": 99,
    "type": "判断题",
    "q": "XPath表达式//div[@class='header'][contains(text(),'Python')] 会选择class='header'且包含文本Python的div标签。",
    "a": "对",
    "desc": "`contains()` 函数用于模糊匹配。"
  },
  {
    "id": 100,
    "type": "判断题",
    "q": "在XPath中，//div[@class='header']/*[2]会选择class='header'的div标签下的第二个子元素，不管它是什么类型的标签。",
    "a": "对",
    "desc": "`*` 通配符匹配任意元素标签，`[2]` 表示位置索引。"
  },
  {
    "id": 101,
    "type": "判断题",
    "q": "soup.select('div p#main') 会选择所有 div 标签中的 id='main' 的 p 标签。",
    "a": "对",
    "desc": "空格表示后代关系，`p#main` 表示 id 为 main 的 p 标签。"
  },
  {
    "id": 102,
    "type": "判断题",
    "q": "soup.select('div .header > p') 会选择所有类名为 header 的div元素的直接子元素 p 。",
    "a": "错",
    "desc": "该选择器表示：div 内部的 -> 类名为 header 的元素 -> 的直接子元素 p。而不是“类名为 header 的 div”。正确的应该是 `div.header > p`。"
  },
  {
    "id": 103,
    "type": "判断题",
    "q": "soup.select('div#header')用于选取页面中第一个id='header'的div标签。",
    "a": "错",
    "desc": "`select()` 返回的是一个**列表**，包含所有符合条件的元素，即使只有一个。"
  },
  {
    "id": 104,
    "type": "判断题",
    "q": "在BeautifulSoup中，soup.find_all('div', {'class': 'header'}) 与soup.find_all('div', class_='header')效果是相同的。",
    "a": "对",
    "desc": "两者都是查找 class 为 header 的 div 的标准写法。"
  },
  {
    "id": 105,
    "type": "判断题",
    "q": "在BeautifulSoup中，.get_text()方法会返回标签内所有文本内容，但不包括任何子标签的内容。",
    "a": "错",
    "desc": "`.get_text()` 会递归获取所有子标签中的文本并拼接。"
  },
  {
    "id": 106,
    "type": "判断题",
    "q": "soup.find('p', {'class': 'intro'}).find('a') 会选取class='intro'的p标签下第一个a标签。",
    "a": "对",
    "desc": "`find()` 返回第一个匹配项，链式调用可以查找后代。"
  },
  {
    "id": 107,
    "type": "判断题",
    "q": "soup.find('div', class_='header').find_all('p') 可以选取class='header'的div标签下所有p标签。",
    "a": "对",
    "desc": ""
  },
  {
    "id": 108,
    "type": "判断题",
    "q": "在BeautifulSoup中，soup.find_all('div', class_='header')[1] 会返回所有class='header'的div标签中的第一个元素。",
    "a": "错",
    "desc": "Python 列表索引从 0 开始，`[1]` 返回的是**第二个**元素。"
  },
  {
    "id": 109,
    "type": "判断题",
    "q": "在BeautifulSoup中，soup.select('div#content p')与soup.find_all('p', {'class': 'content'})效果相同。",
    "a": "错",
    "desc": "前者查找 id='content' 的 div 下的 p；后者查找 class='content' 的 p。逻辑完全不同。"
  },
  {
    "id": 110,
    "type": "判断题",
    "q": "Selenium不支持浏览器的功能，它不需要与第三方浏览器结合使用。",
    "a": "错",
    "desc": "Selenium 必须配合具体的浏览器驱动（如 ChromeDriver）来控制真实的浏览器。"
  },
  {
    "id": 111,
    "type": "判断题",
    "q": "表层网页是指传统搜索引擎可以索引的页面，主要以超链接可以到达的静态网页构成的网页。",
    "a": "对",
    "desc": "相对于深层网页（Deep Web），表层网页是搜索引擎能轻易抓取的。"
  },
  {
    "id": 112,
    "type": "判断题",
    "q": "POST请求的请求参数会暴露在URL地址中。",
    "a": "错",
    "desc": "这是 GET 请求的特征。POST 请求参数在请求体中，不会显示在 URL 里。"
  },
  {
    "id": 113,
    "type": "判断题",
    "q": "JSON比XML的语法更简单，层次结构更加清晰，易于阅读。",
    "a": "对",
    "desc": "JSON 轻量级、易读，是现代 Web 数据交换的主流格式。"
  },
  {
    "id": 114,
    "type": "判断题",
    "q": "使用Selenium可以抓取动态网页中的数据。",
    "a": "对",
    "desc": "这是 Selenium 的核心优势。"
  },
  {
    "id": 115,
    "type": "判断题",
    "q": "Requests是基于urllib3编写的库。",
    "a": "对",
    "desc": "Requests 对 urllib3 进行了封装，提供了更人性化的 API。"
  },
  {
    "id": 116,
    "type": "判断题",
    "q": "服务器端可以记住用户的登录状态，因此HTTP协议自身具有保持会话状态的功能。",
    "a": "错",
    "desc": "HTTP 协议本身是**无状态**的。记住状态靠的是 Cookie/Session 机制，而非协议本身的功能。"
  },
  {
    "id": 117,
    "type": "判断题",
    "q": "Selenium启动浏览器后，浏览器的窗口默认以最大化的形式显示。",
    "a": "错",
    "desc": "默认通常不是最大化，需要调用 `maximize_window()` 方法。"
  },
  {
    "id": 118,
    "type": "判断题",
    "q": "JSONPath只能解析JSON格式的数据。",
    "a": "对",
    "desc": ""
  },
  {
    "id": 119,
    "type": "判断题",
    "q": "re模块在提取HTML标签中的特定信息时比BeautifulSoup 或lxml会更高效。",
    "a": "错",
    "desc": "正则表达式处理简单的字符串匹配快，但在处理复杂的嵌套 HTML 结构时，容易出错且效率不一定优于经过优化的解析库（如 lxml）。"
  },
  {
    "id": 120,
    "type": "判断题",
    "q": "POST请求方法的参数信息会在URL地址中显示。",
    "a": "错",
    "desc": "同第 112 题。"
  },
  {
    "id": 121,
    "type": "判断题",
    "q": "在XPath中，@*表示选取所有属性，而//*表示选取所有节点。",
    "a": "对",
    "desc": ""
  },
  {
    "id": 122,
    "type": "判断题",
    "q": "HTTPS协议在传输数据过程中比HTTP协议更加安全。",
    "a": "对",
    "desc": "HTTPS 加入了 SSL/TLS 层进行加密传输。"
  },
  {
    "id": 123,
    "type": "判断题",
    "q": "JSONPath表达式 $..* 可以用来获取JSON对象中所有字段的值。",
    "a": "对",
    "desc": "`$..*` 表示递归匹配所有成员。"
  },
  {
    "id": 124,
    "type": "判断题",
    "q": "Python中可以使用open()函数将图片数据写入磁盘，其操作模式为'w'。",
    "a": "错",
    "desc": "必须使用 `'wb'` (二进制写入)。"
  },
  {
    "id": 125,
    "type": "判断题",
    "q": "Robots协议可以从根本上约束爬虫程序。",
    "a": "错",
    "desc": "它只是一个文本文件声明，没有技术上的强制阻断能力。"
  },
  {
    "id": 126,
    "type": "判断题",
    "q": "get方法相比post方法能携带更多信息。",
    "a": "错",
    "desc": "POST 支持更大数据量的传输。"
  },
  {
    "id": 127,
    "type": "判断题",
    "q": "XPath中，路径表达式是唯一的。",
    "a": "错",
    "desc": "同一个节点可以通过多种不同的路径（绝对路径、相对路径、属性定位等）来定位。"
  },
  {
    "id": 128,
    "type": "判断题",
    "q": "CSS用于向网页中添加交互行为。",
    "a": "错",
    "desc": "CSS 用于样式和布局，JS (JavaScript) 用于交互行为。"
  },
  {
    "id": 129,
    "type": "判断题",
    "q": "如果服务器返回的状态码为500，则表示客户端发送的请求出现错误。",
    "a": "错",
    "desc": "500 表示**服务器内部错误**；4xx 表示客户端错误。"
  },
  {
    "id": 130,
    "type": "判断题",
    "q": "在使用Selenium时，.get()方法可以加载页面并等待页面完全加载后返回控制权，但不能用于抓取JavaScript渲染的数据。",
    "a": "错",
    "desc": "Selenium 的核心作用就是加载页面并执行 JS，从而获取渲染后的数据。"
  },
  {
    "id": 131,
    "type": "判断题",
    "q": "在实际应用中，文件存储和数据库存储各有利弊，文件存储比较适合中小型网络爬虫，数据库存储比较适合大型网络爬虫。",
    "a": "对",
    "desc": ""
  },
  {
    "id": 132,
    "type": "判断题",
    "q": "线程具有独立运行、状态不可测、执行顺序随机的特点。",
    "a": "对",
    "desc": "多线程的执行由操作系统调度，顺序通常是不确定的。"
  },
  {
    "id": 133,
    "type": "判断题",
    "q": "线程是系统进行资源分配的最小单位。",
    "a": "错",
    "desc": "**进程**是资源分配的最小单位，**线程**是 CPU 调度的最小单位。"
  },
  {
    "id": 134,
    "type": "判断题",
    "q": "线程共享同一进程中的数据。",
    "a": "对",
    "desc": "同一进程下的线程共享内存空间。"
  },
  {
    "id": 135,
    "type": "判断题",
    "q": "一个Scrapy项目可以包含多个爬虫。",
    "a": "对",
    "desc": "Scrapy 项目结构允许在 spiders 目录下创建多个爬虫文件。"
  },
  {
    "id": 136,
    "type": "简答题",
    "q": "请列举并阐述网络爬虫的常见分类。",
    "a": "通用网络爬虫：又称全网爬虫，是指访问全互联网资源的网络爬虫。\n聚焦网络爬虫：又称主题网络爬虫，是指有选择性地访问那些与预定主题相关网页的网络爬虫。\n增量式网络爬虫：是指对已下载的网页采取增量式更新，只抓取新产生或者已经发生变化的网页的网络爬虫。\n深层网络爬虫：是指抓取深层网页的网络爬虫，它要抓取的网页层次比较深，需要通过一定的附加策略才能够自动抓取。",
    "desc": "参考教材第一章爬虫分类相关内容。"
  },
  {
    "id": 137,
    "type": "简答题",
    "q": "XPATH路径表达式由哪些符号组成，请列举并描述它们的含义。",
    "a": "/ :表示从当前节点选择直接子节点。\n// :表示从当前节点选择子孙节点，忽略层级关系。\n. :表示选取当前节点。\n.. :表示选取当前节点的父节点。\n@ :选取属性节点。\n[] :表示谓词",
    "desc": "XPath 基础语法。"
  },
  {
    "id": 138,
    "type": "简答题",
    "q": "请简述浏览器访问百度首页的加载过程。",
    "a": "1. 当我们在地址栏输入百度官网地址后，浏览器首先通过DNS（Domain Name System，域名系统）服\n务器查找百度服务器对应的IP地址；\n2. 接着浏览器向IP地址对应的Web服务器发送HTTP请求；\n3. 然后Web服务器接收HTTP请求后进行处理，向浏览器返回HTML页面；\n4. 最后浏览器对HTML页面进行渲染呈现给用户。",
    "desc": "经典的 DNS 解析与 HTTP 请求响应流程。",
    "opts": {}
  },
  {
    "id": 139,
    "type": "简答题",
    "q": "请简要说明什么是Cookie，它有什么用途？",
    "a": "Cookie 是在用户计算机上存储小型文本文件的技术。\n当用户访问网站时，网站将Cookie发送到用户浏览器存储。\n之后，再次访问时，浏览器会发送Cookie给服务器。用途：识别用户、跟踪会话状态（如登录状态）。",
    "desc": "Cookie 是维持 HTTP 会话状态的关键技术。",
    "opts": {}
  },
  {
    "id": 140,
    "type": "简答题",
    "q": "请求方法GET和POST有哪些主要区别。",
    "a": "GET用于请求服务器发送某个资源，POST用于向服务器提交表单或上传文件，表单数据或文件的数据\n会包含在请求体中。\n（数据大小方面）GET请求方法通过请求参数传输数据，最多只能传输2KB的数据；POST请求方法通过\n实体内容传输数据，可以传输的数据大小没有限制。\n（安全性方面）GET请求方法的参数信息会在URL中明文显示，安全性比较低；POST请求方法传递的参\n数会隐藏在实体内容中，用户看不到，安全性更高。",
    "desc": "HTTP 方法的核心区别。",
    "opts": {}
  },
  {
    "id": 141,
    "type": "简答题",
    "q": "常见的反爬虫措施有哪些，请列举并简述应对方法。",
    "a": "1. 用户身份检查：在请求网页时携带User-Agent，将自己伪装成一个浏览器，如此便可以绕过网站的检\n测，避免出现被网站服务器直接拒绝访问的情况。\n2. IP黑名单（封IP）：\n为防止网站运维人员从访问量上推断出网络爬虫的身份，可以降低网络爬虫访问网站的频率，如让\n网络爬虫每抓取一次页面数据就休息几秒钟，或者限制每天抓取的页面数据的数量。\n网络爬虫在访问网站时，若反复使用同一IP地址进行访问，则极易被网站认出网络爬虫的身份后进\n行屏蔽、阻止、封禁等。此时可以在网络爬虫和Web服务器之间设置代理服务器。\n3. 验证码：有些网站在检测到某个客户端的IP地址访问次数过于频繁时，会要求该客户端进行登录验证，\n并随机提供一个验证码。此时可以使用图像识别等技术识别验证码。\n4. 其他还可以有Cookie验证、网页动态加载、字体反爬、js反爬、数据加密等。",
    "desc": "反爬虫与反反爬虫的对抗。",
    "opts": {}
  },
  {
    "id": 142,
    "type": "简答题",
    "q": "请简述使用requests库进行网页数据抓取的一般流程。",
    "a": "1. 观察网页结构，分析网站与浏览器之间的网络交互，确定包含要抓取的数据的URL。\n2. 确定请求方式，获取请求头信息，分析请求携带的参数信息。使用requests库模拟浏览器向服务器发送\n请求，获取静态页面数据。这个过程可以使用代理IP。\n3. 对获取到的网页源码进行解析，以获取目标数据。\n4. 将得到的数据进行处理，并存放在本地文件或数据库中。",
    "desc": "Requests 爬虫基本五步法。",
    "opts": {}
  },
  {
    "id": 143,
    "type": "简答题",
    "q": "请列举4种用selenium定位单个元素的方法并进行简单说明。",
    "a": "通过ID定位元素driver.find_element(By.ID,\"element_id\")\n通过xpath定位元素driver.find_element(By.XPATH,'//div[@class=\"simple\"]')\n通过CSS选择器定位元素driver.find_element(By.CSS_SELECTOR,\"#element_id\")\n通过标签名定位元素driver.find_element(By.TAG_NAME,\"div\")\n通过'name'属性的值定位元素driver.find_element(By.NAME,\"element_name\")\n通过链接定位元素driver.find_element(By.LINK_TEXT,\"Link Text\")",
    "desc": "Selenium 元素定位策略。",
    "opts": {}
  },
  {
    "id": 144,
    "type": "简答题",
    "q": "请简述robots的作用，并列举和阐述robots.txt文件中的选项。",
    "a": "Robots协议又称爬虫协议，它是国际互联网界通行的道德规范，用于保护网站数据和敏感信息，确保\n网站用户的个人信息和隐私不受侵犯。robots.txt存放于网站的根目录下，文件内容通常包含包含：\nUser-agent：适用对象（用户代理），若该选项的值为“*”，则说明robots.txt文件对任何网络爬虫均有\n效。\nDisallow：不允许访问的目录或文件\nAllow：允许访问的目录或文件\nSitemap：站点地图，告知网络爬虫网站地图的路径，主要说明网站更新时间、更新频率、网址重要程\n度等信息。",
    "desc": "Robots 协议标准。",
    "opts": {}
  },
  {
    "id": 145,
    "type": "简答题",
    "q": "常见的CSS选择器有哪些，请举例说明。",
    "a": "类别选择器：根据类名选择元素，类名前面用“.”进行标注。例如，.intro表示选择包含 class=\"intro\" 的\n所有元素。\nID选择器： 根据特定ID选择元素，ID前面加上 # 进行标注。例如，#link1 表示选择特定ID的值为\nid='link1'的元素。\n属性选择器：根据元素的属性选择元素，属性必须用 [ ] 进行包裹，可以是标准属性 或 自定义属性。例\n如，[target=blank]表示选择包含属性target=\"blank\"的所有元素\n后代选择器：语法：选择器1 选择器2。说明：选择器1和选择器2中间用*空格*隔开，用来选择选择器\n1中的 *子孙元素* （选择器2）。\n子代选择器：语法：选择器1 > 选择器2。说明：用来选择选择器1中的 *直接子元素*（选择器2）。",
    "desc": "CSS Selector 基本语法。",
    "opts": {}
  },
  {
    "id": 146,
    "type": "简答题",
    "q": "请简述多线程爬虫的运行流程。",
    "a": "1. 构建一个网址队列，用于存放网络爬虫待抓取数据的所有网址。\n2. 开启多个线程抓取网页。\n3. 将抓取到的网页源码存储到网页源码队列中。\n4. 开启多个线程对网页源码队列中的网页数据进行解析。\n5. 将解析之后的数据存储到网页数据队列中。\n6. 将网页数据队列中的数据进行存储。",
    "desc": "生产者-消费者模型在爬虫中的应用。",
    "opts": {}
  },
  {
    "id": 147,
    "type": "简答题",
    "q": "请简述文件存储和数据库存储的特点。",
    "a": "文件存储：简单，适合中小型数据，本地存储。\n数据库存储：支持分类、去重、高效检索，适合大型数据。",
    "desc": "数据持久化方案对比。"
  },
  {
    "id": 148,
    "type": "简答题",
    "q": "请简述Scrapy框架的优点和缺点。",
    "a": "优点：（1）具有丰富的文档、良好的社区以及庞大的用户群体。 （2）Scrapy支持并发功能，可以灵\n活地调整并发线程的数量。 （3）采用可读性很强的XPath技术解析网页，解析速度更加快速。 （4）\n具有统一的中间件，可以对数据进行过滤。 （5）支持Shell工具，方便开发人员独立调试程序。 （6）\n通过管道将数据存入数据库，灵活方便，且可以保存为多种形式。 （7）具有高度的可定制化功能，经\n过简单的改造后便可以实现具有特定功能的网络爬虫。\n缺点：（1）自身无法实现分布式爬虫。 （2）去重效果差，极易消耗内存，且不能持久化 。 （3）无\n法获取采用JavaScript技术进行动态渲染的页面内容。",
    "desc": "Scrapy 框架特性分析。",
    "opts": {}
  },
  {
    "id": 149,
    "type": "操作题",
    "q": "编写一个Python程序，使用requests模块发送一个GET请求到http://httpbin.org/get，并打印出相应的text内容。",
    "a": "import requests\nresponse = requests.get('http://httpbin.org/get')\nprint(response.text)",
    "desc": "Requests 基础请求代码。"
  },
  {
    "id": 150,
    "type": "操作题",
    "q": "编写一个Python程序，将一个包含数据的字典写入output.json文件中，并确保输出的JSON字符串是格式化的(缩进为4个单位)。",
    "a": "字典如下：\ndata = {\n   \"title\": \"Sample Data\",\n   \"author\": \"Charlie\",\n   \"publication_year\": 2022,\n   \"genres\": [\"Fiction\", \"Adventure\"]\n}\n参考代码：\nimport json\nwith open('output.json', 'w') as file:\n   json.dump(data, file, indent=4, sort_keys=True)\n'''\n注：参考教材第87页内容。dump()和dumps()方法都用于序列化——把Python对象转换为json字符序列。区别在于\ndumps()方法是仅把数据转换为字符序列，即python对象→字符串。而dump()方法是把数据转换为字符序列后直接保存\n到文件中，因此dump()方法需要多传入一个文件对象作为参数。\n===============================================\njson.dumps() 写法：\njson_str = json.dumps(\n  obj=data,                       # 要转换的数据\n  ensure_ascii=False,             # 允许中文\n  indent=4,                       # 缩进4空格\n  sort_keys=True                 # 按键排序\n)\n================================================\njson.dump() 写法：\nwith open('data.json', 'w') as fp:\n  json.dump(\n      obj=data,                   # 要写入的数据\n      fp=fp,                     # 文件对象（dump特有）\n      ensure_ascii=False,         # 允许中文\n      indent=4,                   # 缩进4空格\n      sort_keys=True             # 按键排序\n3.现有html代码字符串，且已存入html变量中，请分别用XPath和BeautifulSoup对该字符串进行解析，提取第\n一个商品的名称、第二个商品的价格、第二个商品的购买链接分别保存到product1_name, product2_price,\nproduct2_link三个变量中。字符串示例如下：\n1）xpath解析代码。\n2）bs4解析代码。\n4.现有html代码字符串，且已存入html变量中，根据要求完成后续作答。\n  )\n直接写入文件，无返回值\n'''",
    "desc": "JSON 序列化与文件写入。",
    "opts": {}
  },
  {
    "id": 151,
    "type": "操作题",
    "q": ".现有html代码字符串，且已存入html变量中，请分别用XPath和BeautifulSoup对该字符串进行解析，提取第一个商品的名称、第二个商品的价格、第二个商品的购买链接分别保存到product1_name, product2_price,product2_link三个变量中。字符串示例如下：",
    "a": "html = '''\n<html>\n  <body>\n      <div class=\"product\">\n          <h2 class=\"name\">无线耳机</h2>\n          <p class=\"price\">￥199.00</p>\n          <a href=\"https://example.com/product1\">查看详情</a>\n      </div>\n      <div class=\"product\">\n          <h2 class=\"name\">蓝牙音响</h2>\n          <p class=\"price\">￥299.00</p>\n          <a href=\"https://example.com/product2\">查看详情</a>\n      </div>\n  </body>\n</html>\n'''\n1）xpath解析代码。\n2）bs4解析代码。\n4.现有html代码字符串，且已存入html变量中，根据要求完成后续作答。\n  )\n直接写入文件，无返回值\n'''\nhtml = '''\n<html>\n  <body>\n      <div class=\"product\">\n          <h2 class=\"name\">无线耳机</h2>\n          <p class=\"price\">￥199.00</p>\n          <a href=\"https://example.com/product1\">查看详情</a>\n      </div>\n      <div class=\"product\">\n          <h2 class=\"name\">蓝牙音响</h2>\n          <p class=\"price\">￥299.00</p>\n          <a href=\"https://example.com/product2\">查看详情</a>\n      </div>\n  </body>\n</html>\n'''\nfrom lxml import etree\ntree = etree.HTML(html)\nproduct1_name = tree.xpath('//div[@class=\"product\"][1]//h2[@class=\"name\"]/text()')[0]\nproduct2_price = tree.xpath('//div[@class=\"product\"][2]//p[@class=\"price\"]/text()')[0]\nproduct2_link = tree.xpath('//div[@class=\"product\"][2]//a/@href')[0]\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html,’lxml’)\nproduct1_name = soup.select('.product > .name')[0].text\nproduct2_price = soup.select('.product > .price')[1].text\nproduct2_link = soup.select('.product > a')[1].get('href')",
    "desc": "lxml XPath 提取实战。",
    "opts": {}
  },
  {
    "id": 152,
    "type": "操作题",
    "q": "现有html代码字符串，且已存入html变量中，根据要求完成后续作答。",
    "a": "html=\"\"\"\n<table class=\"rk-table\">\n  <tr>\n      <th>排名</th>\n      <th>学校名称</th>\n      <th>省份</th>\n      <th>类型</th>\n  </tr>\n  <tr>\n  <td class=\"rank\">1</td>\n      <td>\n          <div class=\"univname\">\n              <a href=\"...\" class=\"name cn\">清华大学</a>\n              <a href=\"...\" class=\"name en\">Tsinghua University</a>\n              <p class=\"tags\">双一流/985/211</p>\n        </div>\n      </td>\n      <td id=\"address\">北京</td>\n      <td><span>综合</span></td>\n  </tr>\n</table>\n\"\"\"\n1）请分别使用bs4和lxml创建对象解析该字符串\n2）分别使用bs4和lxml获取排名、学校中文名称、省份、类型信息，并分别存入rank、name、address、\ncategory变量中\n5.现有HTML页面源代码并存入html_content变量中，如下所示：\n      <th>省份</th>\n      <th>类型</th>\n  </tr>\n  <tr>\n  <td class=\"rank\">1</td>\n      <td>\n          <div class=\"univname\">\n              <a href=\"...\" class=\"name cn\">清华大学</a>\n              <a href=\"...\" class=\"name en\">Tsinghua University</a>\n              <p class=\"tags\">双一流/985/211</p>\n        </div>\n      </td>\n      <td id=\"address\">北京</td>\n      <td><span>综合</span></td>\n  </tr>\n</table>\n\"\"\"\n#bs4实现\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html,'lxml')\n#lxml实现\nfrom lxml import etree\ntree = etree.HTML(html)\n#bs4实现\nrank = soup.select('.rank')[0].text\nname = soup.select('.name cn')[0].text\naddress = soup.select('#address')[0].text\ncategory = soup.select('span')[0].text\nprint(rank, name, address, category)\n#lxml实现\nrank = tree.xpath('//td[@class=\"rank\"]/text()')[0]\nname = tree.xpath('//a[@class=\"name cn\"]/text()')[0]\naddress = tree.xpath('//td[@id=\"address\"]/text()')[0]\ncategory = tree.xpath('//span/text()')[0]\nprint(rank, name, address, category)",
    "desc": "BS4 CSS 选择器提取实战。",
    "opts": {}
  },
  {
    "id": 153,
    "type": "操作题",
    "q": "5.现有HTML页面源代码并存入html_content变量中，如下所示：",
    "a": "<head>\n  <title>示例页面</title>\n</head>\n<body>\n  <div class=\"content\">\n      <h1>欢迎来到我的网站</h1>\n      <p class=\"description\">这是一个示例页面，用于展示如何使用lxml模块。</p>\n      <ul>\n          <li><a href=\"http://example.com/page1\">页面1</a></li>\n          <li><a href=\"http://example.com/page2\">页面2</a></li>\n          <li><a href=\"http://example.com/page3\">页面3</a></li>\n      </ul>\n  </div>\n</body>\n</html>\n'''",
    "desc": "BS4 提取表格数据。",
    "opts": {}
  },
  {
    "id": 154,
    "type": "操作题",
    "q": "编写一个Selenium脚本，打开指定网页，定位到一个输入框并输入文本，然后点击一个按钮。请根据要求补全代码：",
    "a": "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\n# 初始化WebDriver（以Chrome为例）\ndriver = webdriver.Chrome()\ntry:\n   # 打开指定网页\"http://www.example.com\"\n   driver.get(\"http://www.example.com\")\n   # 假设输入框元素的id为'input_id'，请完成代码定位到输入框并输入文本并输入文本\"Hello, Selenium!\"\n   input_element = driver.find_element(By.ID, \"input_id\")  \n   input_element.send_keys(\"Hello, Selenium!\")\n   # 假设按钮元素的id为'button_id'，请完成代码点击按钮\n   button_element = driver.find_element(By.ID, \"button_id\")\n   button_element.click()\nfinally:\n   # 关闭浏览器\n   driver.quit()",
    "desc": "XPath 提取表格数据。",
    "opts": {}
  },
  {
    "id": 155,
    "type": "操作题",
    "q": "7.目标网站url地址为https://cq.fang.ke.com/loupan/page/1-50，共计50页，页面效果及核心代码如下图所\n示。请结合所学知识，完成以下需求:\n（1）爬取这50个页面的HTML代码；\n（2）从爬取的HTML中，提取出楼盘名称、地址、房间大小、房价并储存到列表中；编写的Python代码如下，\n根据提示选择正确的代码项。\n已知html代码如下所示：\n<ul>\n   <li class=\"list\">\n       <a href=\"detail?id=1\" class=\"pic_link\">\n           <img src=\"image1.jpg\" alt=\"金洲云澜栖\"/>\n       </a>\n       <div class=\"msg_box\">\n           <div class=\"title\">\n               <a href=\"detail?id=1\">金洲云澜栖</a>\n               <span>16.8万元</span>\n           </div>\n           <div class=\"size\">一室 / 50平米</div>\n           <div class=\"address\">重庆.江津</div>\n       </div>\n   </li>\n   <li class=\"list\">结构同上，共20条,...</li>\n</ul>",
    "a": "参考代码：\n   driver.get(\"http://www.example.com\")\n   # 假设输入框元素的id为'input_id'，请完成代码定位到输入框并输入文本并输入文本\"Hello, Selenium!\"\n   input_element = driver.find_element(By.ID, \"input_id\")  \n   input_element.send_keys(\"Hello, Selenium!\")\n   # 假设按钮元素的id为'button_id'，请完成代码点击按钮\n   button_element = driver.find_element(By.ID, \"button_id\")\n   button_element.click()\nfinally:\n   # 关闭浏览器\n   driver.quit()\n<ul>\n   <li class=\"list\">\n       <a href=\"detail?id=1\" class=\"pic_link\">\n           <img src=\"image1.jpg\" alt=\"金洲云澜栖\"/>\n       </a>\n       <div class=\"msg_box\">\n           <div class=\"title\">\n               <a href=\"detail?id=1\">金洲云澜栖</a>\n               <span>16.8万元</span>\n           </div>\n           <div class=\"size\">一室 / 50平米</div>\n           <div class=\"address\">重庆.江津</div>\n       </div>\n   </li>\n   <li class=\"list\">结构同上，共20条,...</li>\n</ul>\nimport requests\nfrom lxml import etree\nimport pandas as pd\ndef get_text(page):  \n   url = f'https://cq.fang.ke.com/loupan/page/{str(page)}/'\n   headers = {\n       'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\n(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n  }\n   response = requests.get(url,headers=headers)\n   text = response.text\n  response = requests.get(url,headers=headers)\n   text = response.text\n   return text\ndef download(text):\n   #创建xpath对象\n   tree = etree.HTML(text)\n   #获取楼盘名称name的列表\n   name_list = tree.xpath('//li[@class=\"list\"]/a/img/@alt')\n   #获取楼盘地址address的列表\n   address_list = tree.xpath('//div[@class=\"address\"]/text()')\n   #获取房间面积size的列表\n   size_list = tree.xpath('//div[@class=\"size\"]/text()')\n   #获取房价price的列表\n   price_list = tree.xpath('//div[@class=\"msg_box\"]/div/span/text()')\n   #创建一个空列表result_list来接受数据\n   result_list = []\n   #通过for循环遍历出所有数据\n   for i in range(len(name_list)): \n       name = name_list[i]\n       address = address_list[i]\n       size = size_list[i]\n       price = price_list[i]\n       #将遍历得到的数据以列表的形式分组添加到列表中\n       result_list.append([name,address,size,price]) \nif __name__ == '__main__': \n   for page in range(1,51): \n       text = get_text(page)\n       download(text)",
    "desc": "XPath 综合练习。",
    "opts": {}
  },
  {
    "id": 156,
    "type": "操作题",
    "q": "编写Selenium脚本:打开http://www.example.com，定位id='input_id'输入框并输入'Hello, Selenium!'，然后点击id='button_id'的按钮。",
    "a": "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome()\ntry:\n    driver.get(\"http://www.example.com\")\n    input_element = driver.find_element(By.ID, \"input_id\")\n    input_element.send_keys(\"Hello, Selenium!\")\n    button_element = driver.find_element(By.ID, \"button_id\")\n    button_element.click()\nfinally:\n    driver.quit()",
    "desc": "Selenium 基础交互：打开、输入、点击。"
  },
  {
    "id": 157,
    "type": "操作题",
    "q": "目标网站共50页，爬取html并提取楼盘名称、地址、面积、房价。请补全参考代码中的download函数部分。",
    "a": "def download(text):\n    tree = etree.HTML(text)\n    name_list = tree.xpath('//li[@class=\"list\"]/a/img/@alt')\n    address_list = tree.xpath('//div[@class=\"address\"]/text()')\n    size_list = tree.xpath('//div[@class=\"size\"]/text()')\n    price_list = tree.xpath('//div[@class=\"msg_box\"]/div/span/text()')\n    result_list = []\n    for i in range(len(name_list)):\n        name = name_list[i]\n        address = address_list[i]\n        size = size_list[i]\n        price = price_list[i]\n        result_list.append([name, address, size, price])",
    "desc": "爬虫数据解析与列表整合。"
  },
  {
    "id": 158,
    "type": "操作题",
    "q": "编写爬虫，爬取新浪新闻头条的标题、时间和链接，写入excel。请补全parse_html函数的逻辑。",
    "a": "def parse_html(text):\n    item = []\n    tree = etree.HTML(text)\n    title_list = tree.xpath('//h2[@class=\"news-title\"]/text()')\n    time_list = tree.xpath('//p[@class=\"news-time\"]/text()')\n    link_list = tree.xpath('//a/@href')\n    for i in range(len(title_list)):\n        title = title_list[i]\n        time = time_list[i]\n        link = link_list[i]\n        item.append({\n            '新闻标题': title,\n            '发布时间': time,\n            '新闻链接': link\n        })\n    return item",
    "desc": "数据结构化提取（字典列表）。"
  },
  {
    "id": 159,
    "type": "操作题",
    "q": "爬取站长素材网风景图片标题和链接，保存到Excel。请补全parse_html函数。",
    "a": "def parse_html(text, item):\n    tree = etree.HTML(text)\n    title_list = tree.xpath('//div[@class=\"item\"]/img/@alt')\n    src_list = tree.xpath('//div[@class=\"item\"]/img/@src')\n    for i in range(len(title_list)):\n        title = title_list[i]\n        src = src_list[i]\n        item.append({\n            'Image Title': title,\n            'Image URL': src\n        })",
    "desc": "图片信息提取实战。"
  },
  {
    "id": 160,
    "type": "简答题",
    "q": "请论述在网络爬虫开发中需要考虑哪些法律与伦理问题？作为一名开发者，应如何遵守相关规范？",
    "a": "参考答案：爬虫开发需严守法律与伦理底线：法律上不得侵犯著作权、个人信息及商业秘密，严禁非法侵入系统或扰乱网站运营；伦理上要尊重网站意愿、节约网络资源，保证数据使用透明公正。开发者应事前做好合规审查，技术上友好爬取、最小化收集数据并妥善保护，事后持续监控行为、响应反馈，关注法规变化，合法审慎运用爬虫技术。",
    "desc": "爬虫合规性与法律边界。",
    "opts": {}
  },
  {
    "id": 161,
    "type": "简答题",
    "q": "请简述你写网络爬虫程序的基本思路或流程，并举例说明爬虫技术在实际应用中的几种常见场景。",
    "a": "流程：抓包分析 -> 获取静态页面(Requests) -> 应对反爬(User-Agent/IP代理) -> HTML解析(XPath/BS4) -> 数据存储。\n应用场景：搜索引擎索引、电商价格监控、舆情分析。",
    "desc": "爬虫开发宏观流程。"
  },
  {
    "id": 162,
    "type": "简答题",
    "q": "这学期学了哪些解析网页数据的技术？请列举并对不同技术的应用场景或者优缺点进行简单分析说明。",
    "a": "XPath: 语法通用，解析速度快，适合结构化文档。\nBeautifulSoup: 容错率高，API简单易用，但速度稍慢，适合处理不规范HTML。\nJSONPath: 专门解析JSON数据，语法简洁。\n正则表达式: 灵活强大，适合提取特定模式的字符串，但编写复杂，容易出错。",
    "desc": "解析库横向对比。"
  },
  {
    "id": 163,
    "type": "简答题",
    "q": "请探讨requests和selenium的技术特性差异，并各举一个典型应用场景。",
    "a": "Requests: 直接发送HTTP请求，速度快，资源消耗少，但无法处理JS渲染的动态内容。适合抓取静态网页或API接口。\nSelenium: 模拟真实浏览器操作，能处理JS渲染和复杂交互，但速度慢，资源消耗大。适合抓取动态加载、需要登录验证的复杂网页。",
    "desc": "静态与动态爬虫技术对比。"
  }
]
